{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization can be achieved using the library NLTK.\n",
    "\n",
    "### Install and Import NLTK\n",
    "Use pip to install NLTK and then import the library to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from NLTK) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from NLTK) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from NLTK) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from NLTK) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import NLTK:\n",
    "When you try to run nltk.download('punkt'), you might get an error regarding SSL certificates. Instead of trying to bypass this certfication requirement try the following:\n",
    "\n",
    "Windows users : Take a look at - /Applications/Python 3.6/Install Certificates.command\n",
    "MAC users: There should be a file named Install Certificates.command in the python application folder, double click it and a script will run.\n",
    "\n",
    "Post this the download should go through smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sahithimv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Sentences\n",
    "To mark the boundaries of sentences in a text we can use a sentence tokenizer. It is important as it is a foundational step of analyzing text at the sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Messi is the best Footballer in the world.', 'He is an inspiration to all footballers.', 'Goals win games,gameplay wins seasons.', 'Messi has scored a whoping 672 goals and 269 assists for @FCBarcelona!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Messi is the best Footballer in the world. He is an inspiration to all footballers. Goals win games,gameplay wins seasons. Messi has scored a whoping 672 goals and 269 assists for @FCBarcelona!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words\n",
    "You can tokenize words from the sentence for further processing. \n",
    "(Pick words from tokenized sentences if needed - like from the tokenized sentences above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Messi', 'is', 'the', 'best', 'Footballer', 'in', 'the', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "text = \"Messi is the best Footballer in the world.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenization\n",
    "In some cases you might want to come up with a tokenization process that is specific to your use case. For Example - \n",
    "1) Tokenization based on Delimitter - Splits text using special delimitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Custom tokenization', ' using various delimiters', ' such as', ' comma', ' semicolon', ' and colon.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"Custom tokenization, using various delimiters; such as, comma, semicolon, and colon.\"\n",
    "delimiters = [',', ';', ':']\n",
    "pattern = '|'.join(map(re.escape, delimiters))\n",
    "tokens = re.split(pattern, sentence)\n",
    "ans = [token for token in tokens if token]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Tokenization based on length of words - Pick out words based on range of length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'text', 'based', 'word', 'length,']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Tokenize this text based on word length, considering min and max lengths.\"\n",
    "min_length = 4\n",
    "max_length = 7\n",
    "words = sentence.split()\n",
    "ans = [word for word in words if min_length<= len(word) <= max_length]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language specific tokenization\n",
    "While this doesn't happen often, sometimes you might want to deal with tokenization of data that is in different languages. Let's take a look at an example for some Spanish text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Esto', 'es', 'un', 'ejemplo', '.', 'Tokenizarlo', 'con', 'NLTK', '.']\n"
     ]
    }
   ],
   "source": [
    "spanish_text = \"Esto es un ejemplo. Tokenizarlo con NLTK.\"\n",
    "spanish_tokens = word_tokenize(spanish_text, language='spanish')\n",
    "print(spanish_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Abbrievated words\n",
    "Tokenize words such as U.S.A etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S.A.', 'is', 'a', 'country', 'E', 'mc', '2', 'is', 'an', 'equation']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"U.S.A. is a country. E=mc^2 is an equation.\"\n",
    "pattern = re.compile(r'\\b(?:[A-Z]\\.)+|[a-zA-Z0-9-]+')\n",
    "tokens = re.findall(pattern, sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the SpaCy library if you feel it fits your requirements more perfectly. It should be pretty analogous to the above mentioned scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
