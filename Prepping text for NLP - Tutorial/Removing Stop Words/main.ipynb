{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words\n",
    "\n",
    "### Downloads required\n",
    "You need the stop words data from NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sahithimv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sahithimv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words act as data noise, introducing bias to your results. Their removal minimizes the emphasis placed on insignificant elements, such as common words ('the', 'is', 'are'), which do not contribute substantially to the core essence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messi best Footballer world . inspiration footballers . Goals win games , gameplay wins seasons . Messi scored whoping 672 goals 269 assists @ FCBarcelona !\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "text = \"Messi is the best Footballer in the world. He is an inspiration to all footballers. Goals win games,gameplay wins seasons. Messi has scored a whoping 672 goals and 269 assists for @FCBarcelona!\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "processed_text = ' '.join(filtered_words)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in earlier examples you can use this with different languages, for example - Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejemplo . Tokenizarlo NLTK .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "text = \"Esto es un ejemplo. Tokenizarlo con NLTK.\"\n",
    "stop_words = set(stopwords.words('spanish')) # <-- Plug in Language here.\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "processed_text = ' '.join(filtered_words)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the SpaCy library if you feel it fits your requirements more perfectly. It should be pretty analogous to the above mentioned scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
